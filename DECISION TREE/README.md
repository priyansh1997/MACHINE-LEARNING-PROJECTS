# DECISION TREE 
THIS A SUPERVISED LEARNING ALGO AND CAN BE USED FOR BOTH CLASSIFICATION AS WELL AS REGRESSION PROBLEMS. IT SOLVES THE PROBLEM BY TREE REPRESENTATION WHERE EACH INTERNAL NODE IS
AN ATTRIBUTE AND EACH LEAF NODE IS AN CLASS LABEL.
1. GINI IS A DEFAULT SPLITTING CRITERION
2. ENTROPY IS MEASUREMENT OF PURITY IN DATASET
3. GAIN IS MEASURE OF DECREASE IN ENTROPY AND WE HAVE TO SPLIT WITH HIGHEST GAIN
DECISION TREE ARE BASED ON CART(CLASSIFICATION AND REGRESSION TREE)
CART IS MODIFIED FORM OF ID3.
ID3 STANDS FOR ITERATIVE DICHOTOMISER 3(REPEATEDLY DEVIDING) MEANS DEVIDING INTO 2 OR MORE GROUPS(MAINLY CLASSIFICATION). C4.5 IS AN ADVANCED FORM OF ID3 WHICH CONVERTS CONT. FEATUURES INTO CATEGORICAL AND THEN PROCEED WITH ID3.
CART IS BASED ON C4.5 WHERE THE TARGET CAN BE CONTINUOUS.
1. ENTROPY=SIGMA(Pi(Log(Pi))    (Log OF BASE 2)
2. GINI=1-SIGMA(Pi)^2
